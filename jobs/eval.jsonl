{"training_id": "0f027e23", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 16, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/0f027e23/TorchTrainer_2024-10-03_10-00-04/TorchTrainer_7f868_00000_0_2024-10-03_10-00-04/checkpoint_000000"}
{"training_id": "0f027e23", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 16, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/0f027e23/TorchTrainer_2024-10-03_10-00-04/TorchTrainer_7f868_00000_0_2024-10-03_10-00-04/checkpoint_000004"}
{"training_id": "0f027e23", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 16, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/0f027e23/TorchTrainer_2024-10-03_10-00-04/TorchTrainer_7f868_00000_0_2024-10-03_10-00-04/checkpoint_000003"}
{"training_id": "0f027e23", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 16, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/0f027e23/TorchTrainer_2024-10-03_10-00-04/TorchTrainer_7f868_00000_0_2024-10-03_10-00-04/checkpoint_000001"}
{"training_id": "0f027e23", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 16, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/0f027e23/TorchTrainer_2024-10-03_10-00-04/TorchTrainer_7f868_00000_0_2024-10-03_10-00-04/checkpoint_000002"}
{"training_id": "60eeac79", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/60eeac79/TorchTrainer_2024-10-05_07-39-31/TorchTrainer_31ee8_00000_0_2024-10-05_07-39-31/checkpoint_000001"}
{"training_id": "60eeac79", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/60eeac79/TorchTrainer_2024-10-05_07-39-31/TorchTrainer_31ee8_00000_0_2024-10-05_07-39-31/checkpoint_000002"}
{"training_id": "60eeac79", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/60eeac79/TorchTrainer_2024-10-05_07-39-31/TorchTrainer_31ee8_00000_0_2024-10-05_07-39-31/checkpoint_000000"}
{"training_id": "60eeac79", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/60eeac79/TorchTrainer_2024-10-05_07-39-31/TorchTrainer_31ee8_00000_0_2024-10-05_07-39-31/checkpoint_000004"}
{"training_id": "60eeac79", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/60eeac79/TorchTrainer_2024-10-05_07-39-31/TorchTrainer_31ee8_00000_0_2024-10-05_07-39-31/checkpoint_000003"}
{"training_id": "8a7744d9", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 2, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 32, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/8a7744d9/TorchTrainer_2024-10-03_12-49-01/TorchTrainer_19cf2_00000_0_2024-10-03_12-49-01/checkpoint_000001"}
{"training_id": "8a7744d9", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 2, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 32, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/8a7744d9/TorchTrainer_2024-10-03_12-49-01/TorchTrainer_19cf2_00000_0_2024-10-03_12-49-01/checkpoint_000000"}
{"training_id": "60b8ce63", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/60b8ce63/TorchTrainer_2024-10-06_12-49-32/TorchTrainer_abc7a_00000_0_2024-10-06_12-49-33/checkpoint_000002"}
{"training_id": "60b8ce63", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/60b8ce63/TorchTrainer_2024-10-06_12-49-32/TorchTrainer_abc7a_00000_0_2024-10-06_12-49-33/checkpoint_000001"}
{"training_id": "60b8ce63", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/60b8ce63/TorchTrainer_2024-10-06_12-49-32/TorchTrainer_abc7a_00000_0_2024-10-06_12-49-33/checkpoint_000004"}
{"training_id": "60b8ce63", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/60b8ce63/TorchTrainer_2024-10-06_12-49-32/TorchTrainer_abc7a_00000_0_2024-10-06_12-49-33/checkpoint_000003"}
{"training_id": "60b8ce63", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/60b8ce63/TorchTrainer_2024-10-06_12-49-32/TorchTrainer_abc7a_00000_0_2024-10-06_12-49-33/checkpoint_000000"}
{"training_id": "ebb93830", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 4, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 32, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/ebb93830/TorchTrainer_2024-10-03_15-10-39/TorchTrainer_e2fc2_00000_0_2024-10-03_15-10-39/checkpoint_000003"}
{"training_id": "ebb93830", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 4, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 32, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/ebb93830/TorchTrainer_2024-10-03_15-10-39/TorchTrainer_e2fc2_00000_0_2024-10-03_15-10-39/checkpoint_000000"}
{"training_id": "ebb93830", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 4, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 32, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/ebb93830/TorchTrainer_2024-10-03_15-10-39/TorchTrainer_e2fc2_00000_0_2024-10-03_15-10-39/checkpoint_000002"}
{"training_id": "ebb93830", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 4, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 32, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/ebb93830/TorchTrainer_2024-10-03_15-10-39/TorchTrainer_e2fc2_00000_0_2024-10-03_15-10-39/checkpoint_000001"}
{"training_id": "4493ccc5", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 3, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 16, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/4493ccc5/TorchTrainer_2024-10-03_06-45-37/TorchTrainer_55c47_00000_0_2024-10-03_06-45-37/checkpoint_000000"}
{"training_id": "4493ccc5", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 3, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 16, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/4493ccc5/TorchTrainer_2024-10-03_06-45-37/TorchTrainer_55c47_00000_0_2024-10-03_06-45-37/checkpoint_000001"}
{"training_id": "4493ccc5", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 3, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 16, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/4493ccc5/TorchTrainer_2024-10-03_06-45-37/TorchTrainer_55c47_00000_0_2024-10-03_06-45-37/checkpoint_000002"}
{"training_id": "74643d18", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 1, "num_checkpoints_to_keep": null, "lr": 1e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 8, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/74643d18/TorchTrainer_2024-10-04_13-56-38/TorchTrainer_b657a_00000_0_2024-10-04_13-56-38/checkpoint_000000"}
{"training_id": "0267e62b", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 8, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/0267e62b/TorchTrainer_2024-10-03_03-01-03/TorchTrainer_f6518_00000_0_2024-10-03_03-01-03/checkpoint_000003"}
{"training_id": "0267e62b", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 8, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/0267e62b/TorchTrainer_2024-10-03_03-01-03/TorchTrainer_f6518_00000_0_2024-10-03_03-01-03/checkpoint_000004"}
{"training_id": "0267e62b", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 8, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/0267e62b/TorchTrainer_2024-10-03_03-01-03/TorchTrainer_f6518_00000_0_2024-10-03_03-01-03/checkpoint_000000"}
{"training_id": "0267e62b", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 8, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/0267e62b/TorchTrainer_2024-10-03_03-01-03/TorchTrainer_f6518_00000_0_2024-10-03_03-01-03/checkpoint_000002"}
{"training_id": "0267e62b", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 8, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/0267e62b/TorchTrainer_2024-10-03_03-01-03/TorchTrainer_f6518_00000_0_2024-10-03_03-01-03/checkpoint_000001"}
{"training_id": "25300d12", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/25300d12/TorchTrainer_2024-10-06_14-43-45/TorchTrainer_a0433_00000_0_2024-10-06_14-43-45/checkpoint_000000"}
{"training_id": "56588502", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 4, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 128, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/56588502/TorchTrainer_2024-10-04_00-51-41/TorchTrainer_0e413_00000_0_2024-10-04_00-51-41/checkpoint_000001"}
{"training_id": "56588502", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 4, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 128, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/56588502/TorchTrainer_2024-10-04_00-51-41/TorchTrainer_0e413_00000_0_2024-10-04_00-51-41/checkpoint_000002"}
{"training_id": "56588502", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 4, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 128, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/56588502/TorchTrainer_2024-10-04_00-51-41/TorchTrainer_0e413_00000_0_2024-10-04_00-51-41/checkpoint_000000"}
{"training_id": "56588502", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 4, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 128, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/56588502/TorchTrainer_2024-10-04_00-51-41/TorchTrainer_0e413_00000_0_2024-10-04_00-51-41/checkpoint_000003"}
{"training_id": "1f106c94", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 1, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 16, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/1f106c94/TorchTrainer_2024-10-03_05-18-21/TorchTrainer_24dfc_00000_0_2024-10-03_05-18-21/checkpoint_000000"}
{"training_id": "24fb192a", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/24fb192a/TorchTrainer_2024-10-05_10-31-06/TorchTrainer_2a09b_00000_0_2024-10-05_10-31-06/checkpoint_000004"}
{"training_id": "24fb192a", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/24fb192a/TorchTrainer_2024-10-05_10-31-06/TorchTrainer_2a09b_00000_0_2024-10-05_10-31-06/checkpoint_000003"}
{"training_id": "24fb192a", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/24fb192a/TorchTrainer_2024-10-05_10-31-06/TorchTrainer_2a09b_00000_0_2024-10-05_10-31-06/checkpoint_000000"}
{"training_id": "24fb192a", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/24fb192a/TorchTrainer_2024-10-05_10-31-06/TorchTrainer_2a09b_00000_0_2024-10-05_10-31-06/checkpoint_000002"}
{"training_id": "24fb192a", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/24fb192a/TorchTrainer_2024-10-05_10-31-06/TorchTrainer_2a09b_00000_0_2024-10-05_10-31-06/checkpoint_000001"}
{"training_id": "cf8ff955", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/cf8ff955/TorchTrainer_2024-10-05_16-38-14/TorchTrainer_73caf_00000_0_2024-10-05_16-38-14/checkpoint_000003"}
{"training_id": "cf8ff955", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/cf8ff955/TorchTrainer_2024-10-05_16-38-14/TorchTrainer_73caf_00000_0_2024-10-05_16-38-14/checkpoint_000004"}
{"training_id": "cf8ff955", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/cf8ff955/TorchTrainer_2024-10-05_16-38-14/TorchTrainer_73caf_00000_0_2024-10-05_16-38-14/checkpoint_000000"}
{"training_id": "cf8ff955", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/cf8ff955/TorchTrainer_2024-10-05_16-38-14/TorchTrainer_73caf_00000_0_2024-10-05_16-38-14/checkpoint_000002"}
{"training_id": "cf8ff955", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/cf8ff955/TorchTrainer_2024-10-05_16-38-14/TorchTrainer_73caf_00000_0_2024-10-05_16-38-14/checkpoint_000001"}
{"training_id": "ac7aa0fa", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 32, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/ac7aa0fa/TorchTrainer_2024-10-03_17-01-45/TorchTrainer_680dc_00000_0_2024-10-03_17-01-45/checkpoint_000000"}
{"training_id": "ac7aa0fa", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 32, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/ac7aa0fa/TorchTrainer_2024-10-03_17-01-45/TorchTrainer_680dc_00000_0_2024-10-03_17-01-45/checkpoint_000004"}
{"training_id": "ac7aa0fa", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 32, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/ac7aa0fa/TorchTrainer_2024-10-03_17-01-45/TorchTrainer_680dc_00000_0_2024-10-03_17-01-45/checkpoint_000003"}
{"training_id": "ac7aa0fa", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 32, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/ac7aa0fa/TorchTrainer_2024-10-03_17-01-45/TorchTrainer_680dc_00000_0_2024-10-03_17-01-45/checkpoint_000001"}
{"training_id": "ac7aa0fa", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 32, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/ac7aa0fa/TorchTrainer_2024-10-03_17-01-45/TorchTrainer_680dc_00000_0_2024-10-03_17-01-45/checkpoint_000002"}
{"training_id": "3a124347", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/3a124347/TorchTrainer_2024-10-05_16-35-34/TorchTrainer_14884_00000_0_2024-10-05_16-35-34/checkpoint_000003"}
{"training_id": "3a124347", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/3a124347/TorchTrainer_2024-10-05_16-35-34/TorchTrainer_14884_00000_0_2024-10-05_16-35-34/checkpoint_000004"}
{"training_id": "3a124347", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/3a124347/TorchTrainer_2024-10-05_16-35-34/TorchTrainer_14884_00000_0_2024-10-05_16-35-34/checkpoint_000000"}
{"training_id": "3a124347", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/3a124347/TorchTrainer_2024-10-05_16-35-34/TorchTrainer_14884_00000_0_2024-10-05_16-35-34/checkpoint_000002"}
{"training_id": "3a124347", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/3a124347/TorchTrainer_2024-10-05_16-35-34/TorchTrainer_14884_00000_0_2024-10-05_16-35-34/checkpoint_000001"}
{"training_id": "edbce493", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/edbce493/TorchTrainer_2024-10-05_13-35-21/TorchTrainer_e77b4_00000_0_2024-10-05_13-35-21/checkpoint_000000"}
{"training_id": "edbce493", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/edbce493/TorchTrainer_2024-10-05_13-35-21/TorchTrainer_e77b4_00000_0_2024-10-05_13-35-21/checkpoint_000003"}
{"training_id": "edbce493", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/edbce493/TorchTrainer_2024-10-05_13-35-21/TorchTrainer_e77b4_00000_0_2024-10-05_13-35-21/checkpoint_000004"}
{"training_id": "edbce493", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/edbce493/TorchTrainer_2024-10-05_13-35-21/TorchTrainer_e77b4_00000_0_2024-10-05_13-35-21/checkpoint_000001"}
{"training_id": "edbce493", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/edbce493/TorchTrainer_2024-10-05_13-35-21/TorchTrainer_e77b4_00000_0_2024-10-05_13-35-21/checkpoint_000002"}
{"training_id": "93066ad9", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/93066ad9/TorchTrainer_2024-10-06_01-30-44/TorchTrainer_d7ef5_00000_0_2024-10-06_01-30-44/checkpoint_000002"}
{"training_id": "93066ad9", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/93066ad9/TorchTrainer_2024-10-06_01-30-44/TorchTrainer_d7ef5_00000_0_2024-10-06_01-30-44/checkpoint_000001"}
{"training_id": "93066ad9", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/93066ad9/TorchTrainer_2024-10-06_01-30-44/TorchTrainer_d7ef5_00000_0_2024-10-06_01-30-44/checkpoint_000004"}
{"training_id": "93066ad9", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/93066ad9/TorchTrainer_2024-10-06_01-30-44/TorchTrainer_d7ef5_00000_0_2024-10-06_01-30-44/checkpoint_000003"}
{"training_id": "93066ad9", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/93066ad9/TorchTrainer_2024-10-06_01-30-44/TorchTrainer_d7ef5_00000_0_2024-10-06_01-30-44/checkpoint_000000"}
{"training_id": "5c572bba", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/5c572bba/TorchTrainer_2024-10-06_11-40-14/TorchTrainer_fcd9a_00000_0_2024-10-06_11-40-14/checkpoint_000003"}
{"training_id": "5c572bba", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/5c572bba/TorchTrainer_2024-10-06_11-40-14/TorchTrainer_fcd9a_00000_0_2024-10-06_11-40-14/checkpoint_000004"}
{"training_id": "5c572bba", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/5c572bba/TorchTrainer_2024-10-06_11-40-14/TorchTrainer_fcd9a_00000_0_2024-10-06_11-40-14/checkpoint_000000"}
{"training_id": "5c572bba", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/5c572bba/TorchTrainer_2024-10-06_11-40-14/TorchTrainer_fcd9a_00000_0_2024-10-06_11-40-14/checkpoint_000002"}
{"training_id": "5c572bba", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/5c572bba/TorchTrainer_2024-10-06_11-40-14/TorchTrainer_fcd9a_00000_0_2024-10-06_11-40-14/checkpoint_000001"}
{"training_id": "a839476d", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 2, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 8, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/a839476d/TorchTrainer_2024-10-02_22-49-30/TorchTrainer_d2789_00000_0_2024-10-02_22-49-30/checkpoint_000000"}
{"training_id": "a839476d", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 2, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 8, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/a839476d/TorchTrainer_2024-10-02_22-49-30/TorchTrainer_d2789_00000_0_2024-10-02_22-49-30/checkpoint_000001"}
{"training_id": "11c8b5f5", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/11c8b5f5/TorchTrainer_2024-10-05_22-25-30/TorchTrainer_f7239_00000_0_2024-10-05_22-25-30/checkpoint_000001"}
{"training_id": "11c8b5f5", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/11c8b5f5/TorchTrainer_2024-10-05_22-25-30/TorchTrainer_f7239_00000_0_2024-10-05_22-25-30/checkpoint_000002"}
{"training_id": "11c8b5f5", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/11c8b5f5/TorchTrainer_2024-10-05_22-25-30/TorchTrainer_f7239_00000_0_2024-10-05_22-25-30/checkpoint_000000"}
{"training_id": "11c8b5f5", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/11c8b5f5/TorchTrainer_2024-10-05_22-25-30/TorchTrainer_f7239_00000_0_2024-10-05_22-25-30/checkpoint_000003"}
{"training_id": "11c8b5f5", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/11c8b5f5/TorchTrainer_2024-10-05_22-25-30/TorchTrainer_f7239_00000_0_2024-10-05_22-25-30/checkpoint_000004"}
{"training_id": "889ae237", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/889ae237/TorchTrainer_2024-10-05_19-48-53/TorchTrainer_166f3_00000_0_2024-10-05_19-48-53/checkpoint_000000"}
{"training_id": "889ae237", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/889ae237/TorchTrainer_2024-10-05_19-48-53/TorchTrainer_166f3_00000_0_2024-10-05_19-48-53/checkpoint_000003"}
{"training_id": "889ae237", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/889ae237/TorchTrainer_2024-10-05_19-48-53/TorchTrainer_166f3_00000_0_2024-10-05_19-48-53/checkpoint_000004"}
{"training_id": "889ae237", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/889ae237/TorchTrainer_2024-10-05_19-48-53/TorchTrainer_166f3_00000_0_2024-10-05_19-48-53/checkpoint_000001"}
{"training_id": "889ae237", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/889ae237/TorchTrainer_2024-10-05_19-48-53/TorchTrainer_166f3_00000_0_2024-10-05_19-48-53/checkpoint_000002"}
{"training_id": "2157f745", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 1, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 32, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/2157f745/TorchTrainer_2024-10-03_12-19-20/TorchTrainer_f3fa9_00000_0_2024-10-03_12-19-20/checkpoint_000000"}
{"training_id": "7a0db3e9", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 3, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 8, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/7a0db3e9/TorchTrainer_2024-10-02_23-45-28/TorchTrainer_a416c_00000_0_2024-10-02_23-45-28/checkpoint_000000"}
{"training_id": "7a0db3e9", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 3, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 8, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/7a0db3e9/TorchTrainer_2024-10-02_23-45-28/TorchTrainer_a416c_00000_0_2024-10-02_23-45-28/checkpoint_000002"}
{"training_id": "7a0db3e9", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 3, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 8, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/7a0db3e9/TorchTrainer_2024-10-02_23-45-28/TorchTrainer_a416c_00000_0_2024-10-02_23-45-28/checkpoint_000001"}
{"training_id": "c2e6601e", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/c2e6601e/TorchTrainer_2024-10-06_04-20-51/TorchTrainer_9baa4_00000_0_2024-10-06_04-20-51/checkpoint_000002"}
{"training_id": "c2e6601e", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/c2e6601e/TorchTrainer_2024-10-06_04-20-51/TorchTrainer_9baa4_00000_0_2024-10-06_04-20-51/checkpoint_000001"}
{"training_id": "c2e6601e", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/c2e6601e/TorchTrainer_2024-10-06_04-20-51/TorchTrainer_9baa4_00000_0_2024-10-06_04-20-51/checkpoint_000004"}
{"training_id": "c2e6601e", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/c2e6601e/TorchTrainer_2024-10-06_04-20-51/TorchTrainer_9baa4_00000_0_2024-10-06_04-20-51/checkpoint_000003"}
{"training_id": "c2e6601e", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/c2e6601e/TorchTrainer_2024-10-06_04-20-51/TorchTrainer_9baa4_00000_0_2024-10-06_04-20-51/checkpoint_000000"}
{"training_id": "eebe52ee", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 3, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 32, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/eebe52ee/TorchTrainer_2024-10-03_13-45-28/TorchTrainer_fca0c_00000_0_2024-10-03_13-45-28/checkpoint_000000"}
{"training_id": "eebe52ee", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 3, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 32, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/eebe52ee/TorchTrainer_2024-10-03_13-45-28/TorchTrainer_fca0c_00000_0_2024-10-03_13-45-28/checkpoint_000002"}
{"training_id": "eebe52ee", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 3, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 32, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/eebe52ee/TorchTrainer_2024-10-03_13-45-28/TorchTrainer_fca0c_00000_0_2024-10-03_13-45-28/checkpoint_000001"}
{"training_id": "c123bed2", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 128, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/c123bed2/TorchTrainer_2024-10-04_02-52-20/TorchTrainer_e92d0_00000_0_2024-10-04_02-52-20/checkpoint_000001"}
{"training_id": "c123bed2", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 128, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/c123bed2/TorchTrainer_2024-10-04_02-52-20/TorchTrainer_e92d0_00000_0_2024-10-04_02-52-20/checkpoint_000002"}
{"training_id": "c123bed2", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 128, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/c123bed2/TorchTrainer_2024-10-04_02-52-20/TorchTrainer_e92d0_00000_0_2024-10-04_02-52-20/checkpoint_000000"}
{"training_id": "c123bed2", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 128, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/c123bed2/TorchTrainer_2024-10-04_02-52-20/TorchTrainer_e92d0_00000_0_2024-10-04_02-52-20/checkpoint_000004"}
{"training_id": "c123bed2", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 128, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/c123bed2/TorchTrainer_2024-10-04_02-52-20/TorchTrainer_e92d0_00000_0_2024-10-04_02-52-20/checkpoint_000003"}
{"training_id": "2c07ec19", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 3, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/2c07ec19/TorchTrainer_2024-10-04_07-09-11/TorchTrainer_cb09b_00000_0_2024-10-04_07-09-11/checkpoint_000002"}
{"training_id": "2c07ec19", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 3, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/2c07ec19/TorchTrainer_2024-10-04_07-09-11/TorchTrainer_cb09b_00000_0_2024-10-04_07-09-11/checkpoint_000001"}
{"training_id": "2c07ec19", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 3, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/2c07ec19/TorchTrainer_2024-10-04_07-09-11/TorchTrainer_cb09b_00000_0_2024-10-04_07-09-11/checkpoint_000000"}
{"training_id": "035aa994", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 2, "num_checkpoints_to_keep": null, "lr": 1e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 8, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/035aa994/TorchTrainer_2024-10-04_14-25-54/TorchTrainer_ccbf6_00000_0_2024-10-04_14-25-54/checkpoint_000000"}
{"training_id": "dcc36093", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/dcc36093/TorchTrainer_2024-10-05_19-25-32/TorchTrainer_d32ed_00000_0_2024-10-05_19-25-32/checkpoint_000002"}
{"training_id": "dcc36093", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/dcc36093/TorchTrainer_2024-10-05_19-25-32/TorchTrainer_d32ed_00000_0_2024-10-05_19-25-32/checkpoint_000001"}
{"training_id": "dcc36093", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/dcc36093/TorchTrainer_2024-10-05_19-25-32/TorchTrainer_d32ed_00000_0_2024-10-05_19-25-32/checkpoint_000004"}
{"training_id": "dcc36093", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/dcc36093/TorchTrainer_2024-10-05_19-25-32/TorchTrainer_d32ed_00000_0_2024-10-05_19-25-32/checkpoint_000003"}
{"training_id": "dcc36093", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/dcc36093/TorchTrainer_2024-10-05_19-25-32/TorchTrainer_d32ed_00000_0_2024-10-05_19-25-32/checkpoint_000000"}
{"training_id": "36b3973d", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 1, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/36b3973d/TorchTrainer_2024-10-04_05-22-23/TorchTrainer_df203_00000_0_2024-10-04_05-22-23/checkpoint_000000"}
{"training_id": "8546ee9f", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 3, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 128, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/8546ee9f/TorchTrainer_2024-10-03_23-20-43/TorchTrainer_5934b_00000_0_2024-10-03_23-20-43/checkpoint_000000"}
{"training_id": "8546ee9f", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 3, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 128, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/8546ee9f/TorchTrainer_2024-10-03_23-20-43/TorchTrainer_5934b_00000_0_2024-10-03_23-20-43/checkpoint_000001"}
{"training_id": "8546ee9f", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 3, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 128, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/8546ee9f/TorchTrainer_2024-10-03_23-20-43/TorchTrainer_5934b_00000_0_2024-10-03_23-20-43/checkpoint_000002"}
{"training_id": "b1048722", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 4, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/b1048722/TorchTrainer_2024-10-04_08-51-04/TorchTrainer_067fb_00000_0_2024-10-04_08-51-04/checkpoint_000001"}
{"training_id": "b1048722", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 4, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/b1048722/TorchTrainer_2024-10-04_08-51-04/TorchTrainer_067fb_00000_0_2024-10-04_08-51-04/checkpoint_000002"}
{"training_id": "b1048722", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 4, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/b1048722/TorchTrainer_2024-10-04_08-51-04/TorchTrainer_067fb_00000_0_2024-10-04_08-51-04/checkpoint_000000"}
{"training_id": "b1048722", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 4, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/b1048722/TorchTrainer_2024-10-04_08-51-04/TorchTrainer_067fb_00000_0_2024-10-04_08-51-04/checkpoint_000003"}
{"training_id": "c962ec65", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/c962ec65/TorchTrainer_2024-10-05_13-34-33/TorchTrainer_cb00b_00000_0_2024-10-05_13-34-33/checkpoint_000001"}
{"training_id": "c962ec65", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/c962ec65/TorchTrainer_2024-10-05_13-34-33/TorchTrainer_cb00b_00000_0_2024-10-05_13-34-33/checkpoint_000002"}
{"training_id": "c962ec65", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/c962ec65/TorchTrainer_2024-10-05_13-34-33/TorchTrainer_cb00b_00000_0_2024-10-05_13-34-33/checkpoint_000000"}
{"training_id": "c962ec65", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/c962ec65/TorchTrainer_2024-10-05_13-34-33/TorchTrainer_cb00b_00000_0_2024-10-05_13-34-33/checkpoint_000004"}
{"training_id": "c962ec65", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/c962ec65/TorchTrainer_2024-10-05_13-34-33/TorchTrainer_cb00b_00000_0_2024-10-05_13-34-33/checkpoint_000003"}
{"training_id": "cad33f9c", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/cad33f9c/TorchTrainer_2024-10-06_07-14-16/TorchTrainer_d5a21_00000_0_2024-10-06_07-14-16/checkpoint_000000"}
{"training_id": "cad33f9c", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/cad33f9c/TorchTrainer_2024-10-06_07-14-16/TorchTrainer_d5a21_00000_0_2024-10-06_07-14-16/checkpoint_000003"}
{"training_id": "cad33f9c", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/cad33f9c/TorchTrainer_2024-10-06_07-14-16/TorchTrainer_d5a21_00000_0_2024-10-06_07-14-16/checkpoint_000004"}
{"training_id": "cad33f9c", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/cad33f9c/TorchTrainer_2024-10-06_07-14-16/TorchTrainer_d5a21_00000_0_2024-10-06_07-14-16/checkpoint_000001"}
{"training_id": "cad33f9c", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/cad33f9c/TorchTrainer_2024-10-06_07-14-16/TorchTrainer_d5a21_00000_0_2024-10-06_07-14-16/checkpoint_000002"}
{"training_id": "13b4056b", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/13b4056b/TorchTrainer_2024-10-05_01-47-02/TorchTrainer_f418a_00000_0_2024-10-05_01-47-02/checkpoint_000002"}
{"training_id": "13b4056b", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/13b4056b/TorchTrainer_2024-10-05_01-47-02/TorchTrainer_f418a_00000_0_2024-10-05_01-47-02/checkpoint_000001"}
{"training_id": "13b4056b", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/13b4056b/TorchTrainer_2024-10-05_01-47-02/TorchTrainer_f418a_00000_0_2024-10-05_01-47-02/checkpoint_000003"}
{"training_id": "13b4056b", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/13b4056b/TorchTrainer_2024-10-05_01-47-02/TorchTrainer_f418a_00000_0_2024-10-05_01-47-02/checkpoint_000004"}
{"training_id": "13b4056b", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/13b4056b/TorchTrainer_2024-10-05_01-47-02/TorchTrainer_f418a_00000_0_2024-10-05_01-47-02/checkpoint_000000"}
{"training_id": "b9db0e15", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 2, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/b9db0e15/TorchTrainer_2024-10-04_06-00-42/TorchTrainer_39705_00000_0_2024-10-04_06-00-42/checkpoint_000000"}
{"training_id": "b9db0e15", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 2, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/b9db0e15/TorchTrainer_2024-10-04_06-00-42/TorchTrainer_39705_00000_0_2024-10-04_06-00-42/checkpoint_000001"}
{"training_id": "83fd9d21", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/83fd9d21/TorchTrainer_2024-10-06_04-45-41/TorchTrainer_138a8_00000_0_2024-10-06_04-45-41/checkpoint_000004"}
{"training_id": "83fd9d21", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/83fd9d21/TorchTrainer_2024-10-06_04-45-41/TorchTrainer_138a8_00000_0_2024-10-06_04-45-41/checkpoint_000003"}
{"training_id": "83fd9d21", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/83fd9d21/TorchTrainer_2024-10-06_04-45-41/TorchTrainer_138a8_00000_0_2024-10-06_04-45-41/checkpoint_000000"}
{"training_id": "83fd9d21", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/83fd9d21/TorchTrainer_2024-10-06_04-45-41/TorchTrainer_138a8_00000_0_2024-10-06_04-45-41/checkpoint_000002"}
{"training_id": "83fd9d21", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/83fd9d21/TorchTrainer_2024-10-06_04-45-41/TorchTrainer_138a8_00000_0_2024-10-06_04-45-41/checkpoint_000001"}
{"training_id": "e4c4be68", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/e4c4be68/TorchTrainer_2024-10-05_22-46-09/TorchTrainer_d9afe_00000_0_2024-10-05_22-46-09/checkpoint_000004"}
{"training_id": "e4c4be68", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/e4c4be68/TorchTrainer_2024-10-05_22-46-09/TorchTrainer_d9afe_00000_0_2024-10-05_22-46-09/checkpoint_000003"}
{"training_id": "e4c4be68", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/e4c4be68/TorchTrainer_2024-10-05_22-46-09/TorchTrainer_d9afe_00000_0_2024-10-05_22-46-09/checkpoint_000000"}
{"training_id": "e4c4be68", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/e4c4be68/TorchTrainer_2024-10-05_22-46-09/TorchTrainer_d9afe_00000_0_2024-10-05_22-46-09/checkpoint_000002"}
{"training_id": "e4c4be68", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/e4c4be68/TorchTrainer_2024-10-05_22-46-09/TorchTrainer_d9afe_00000_0_2024-10-05_22-46-09/checkpoint_000001"}
{"training_id": "62637ba9", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 1, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 8, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/62637ba9/TorchTrainer_2024-10-02_22-20-51/TorchTrainer_d1ed6_00000_0_2024-10-02_22-20-51/checkpoint_000000"}
{"training_id": "95aba0a7", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 2, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 16, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/95aba0a7/TorchTrainer_2024-10-03_05-47-33/TorchTrainer_38ebc_00000_0_2024-10-03_05-47-33/checkpoint_000001"}
{"training_id": "95aba0a7", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 2, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 16, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/95aba0a7/TorchTrainer_2024-10-03_05-47-33/TorchTrainer_38ebc_00000_0_2024-10-03_05-47-33/checkpoint_000000"}
{"training_id": "f2a00338", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/f2a00338/TorchTrainer_2024-10-05_07-39-44/TorchTrainer_397e3_00000_0_2024-10-05_07-39-44/checkpoint_000004"}
{"training_id": "f2a00338", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/f2a00338/TorchTrainer_2024-10-05_07-39-44/TorchTrainer_397e3_00000_0_2024-10-05_07-39-44/checkpoint_000003"}
{"training_id": "f2a00338", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/f2a00338/TorchTrainer_2024-10-05_07-39-44/TorchTrainer_397e3_00000_0_2024-10-05_07-39-44/checkpoint_000000"}
{"training_id": "f2a00338", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/f2a00338/TorchTrainer_2024-10-05_07-39-44/TorchTrainer_397e3_00000_0_2024-10-05_07-39-44/checkpoint_000002"}
{"training_id": "f2a00338", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/f2a00338/TorchTrainer_2024-10-05_07-39-44/TorchTrainer_397e3_00000_0_2024-10-05_07-39-44/checkpoint_000001"}
{"training_id": "0dadb794", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/0dadb794/TorchTrainer_2024-10-04_11-06-16/TorchTrainer_e95c3_00000_0_2024-10-04_11-06-16/checkpoint_000002"}
{"training_id": "0dadb794", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/0dadb794/TorchTrainer_2024-10-04_11-06-16/TorchTrainer_e95c3_00000_0_2024-10-04_11-06-16/checkpoint_000001"}
{"training_id": "0dadb794", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/0dadb794/TorchTrainer_2024-10-04_11-06-16/TorchTrainer_e95c3_00000_0_2024-10-04_11-06-16/checkpoint_000003"}
{"training_id": "0dadb794", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/0dadb794/TorchTrainer_2024-10-04_11-06-16/TorchTrainer_e95c3_00000_0_2024-10-04_11-06-16/checkpoint_000004"}
{"training_id": "0dadb794", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/0dadb794/TorchTrainer_2024-10-04_11-06-16/TorchTrainer_e95c3_00000_0_2024-10-04_11-06-16/checkpoint_000000"}
{"training_id": "f85e6729", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/f85e6729/TorchTrainer_2024-10-06_01-39-28/TorchTrainer_1045a_00000_0_2024-10-06_01-39-28/checkpoint_000000"}
{"training_id": "f85e6729", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/f85e6729/TorchTrainer_2024-10-06_01-39-28/TorchTrainer_1045a_00000_0_2024-10-06_01-39-28/checkpoint_000004"}
{"training_id": "f85e6729", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/f85e6729/TorchTrainer_2024-10-06_01-39-28/TorchTrainer_1045a_00000_0_2024-10-06_01-39-28/checkpoint_000003"}
{"training_id": "f85e6729", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/f85e6729/TorchTrainer_2024-10-06_01-39-28/TorchTrainer_1045a_00000_0_2024-10-06_01-39-28/checkpoint_000001"}
{"training_id": "f85e6729", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/f85e6729/TorchTrainer_2024-10-06_01-39-28/TorchTrainer_1045a_00000_0_2024-10-06_01-39-28/checkpoint_000002"}
{"training_id": "99f4fa22", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 1, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 128, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/99f4fa22/TorchTrainer_2024-10-03_21-47-59/TorchTrainer_64ba7_00000_0_2024-10-03_21-47-59/checkpoint_000000"}
{"training_id": "1576d0f5", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/1576d0f5/TorchTrainer_2024-10-05_10-35-49/TorchTrainer_d2ea1_00000_0_2024-10-05_10-35-49/checkpoint_000002"}
{"training_id": "1576d0f5", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/1576d0f5/TorchTrainer_2024-10-05_10-35-49/TorchTrainer_d2ea1_00000_0_2024-10-05_10-35-49/checkpoint_000001"}
{"training_id": "1576d0f5", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/1576d0f5/TorchTrainer_2024-10-05_10-35-49/TorchTrainer_d2ea1_00000_0_2024-10-05_10-35-49/checkpoint_000003"}
{"training_id": "1576d0f5", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/1576d0f5/TorchTrainer_2024-10-05_10-35-49/TorchTrainer_d2ea1_00000_0_2024-10-05_10-35-49/checkpoint_000004"}
{"training_id": "1576d0f5", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/1576d0f5/TorchTrainer_2024-10-05_10-35-49/TorchTrainer_d2ea1_00000_0_2024-10-05_10-35-49/checkpoint_000000"}
{"training_id": "6dce3431", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/6dce3431/TorchTrainer_2024-10-05_04-36-48/TorchTrainer_ab8da_00000_0_2024-10-05_04-36-48/checkpoint_000000"}
{"training_id": "6dce3431", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/6dce3431/TorchTrainer_2024-10-05_04-36-48/TorchTrainer_ab8da_00000_0_2024-10-05_04-36-48/checkpoint_000003"}
{"training_id": "6dce3431", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/6dce3431/TorchTrainer_2024-10-05_04-36-48/TorchTrainer_ab8da_00000_0_2024-10-05_04-36-48/checkpoint_000004"}
{"training_id": "6dce3431", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/6dce3431/TorchTrainer_2024-10-05_04-36-48/TorchTrainer_ab8da_00000_0_2024-10-05_04-36-48/checkpoint_000001"}
{"training_id": "6dce3431", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/6dce3431/TorchTrainer_2024-10-05_04-36-48/TorchTrainer_ab8da_00000_0_2024-10-05_04-36-48/checkpoint_000002"}
{"training_id": "5fdfa095", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/5fdfa095/TorchTrainer_2024-10-06_10-03-28/TorchTrainer_7855f_00000_0_2024-10-06_10-03-28/checkpoint_000004"}
{"training_id": "5fdfa095", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/5fdfa095/TorchTrainer_2024-10-06_10-03-28/TorchTrainer_7855f_00000_0_2024-10-06_10-03-28/checkpoint_000003"}
{"training_id": "5fdfa095", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/5fdfa095/TorchTrainer_2024-10-06_10-03-28/TorchTrainer_7855f_00000_0_2024-10-06_10-03-28/checkpoint_000000"}
{"training_id": "5fdfa095", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/5fdfa095/TorchTrainer_2024-10-06_10-03-28/TorchTrainer_7855f_00000_0_2024-10-06_10-03-28/checkpoint_000002"}
{"training_id": "5fdfa095", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/5fdfa095/TorchTrainer_2024-10-06_10-03-28/TorchTrainer_7855f_00000_0_2024-10-06_10-03-28/checkpoint_000001"}
{"training_id": "2efe2694", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 4, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 16, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/2efe2694/TorchTrainer_2024-10-03_08-09-01/TorchTrainer_fc1b6_00000_0_2024-10-03_08-09-01/checkpoint_000001"}
{"training_id": "2efe2694", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 4, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 16, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/2efe2694/TorchTrainer_2024-10-03_08-09-01/TorchTrainer_fc1b6_00000_0_2024-10-03_08-09-01/checkpoint_000002"}
{"training_id": "2efe2694", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 4, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 16, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/2efe2694/TorchTrainer_2024-10-03_08-09-01/TorchTrainer_fc1b6_00000_0_2024-10-03_08-09-01/checkpoint_000000"}
{"training_id": "2efe2694", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 4, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 16, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/2efe2694/TorchTrainer_2024-10-03_08-09-01/TorchTrainer_fc1b6_00000_0_2024-10-03_08-09-01/checkpoint_000003"}
{"training_id": "a34ccaf5", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 2, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 128, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/a34ccaf5/TorchTrainer_2024-10-03_22-19-35/TorchTrainer_cebc2_00000_0_2024-10-03_22-19-35/checkpoint_000001"}
{"training_id": "a34ccaf5", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 2, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 128, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/a34ccaf5/TorchTrainer_2024-10-03_22-19-35/TorchTrainer_cebc2_00000_0_2024-10-03_22-19-35/checkpoint_000000"}
{"training_id": "4b4ede7f", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 4, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 8, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/4b4ede7f/TorchTrainer_2024-10-03_01-11-04/TorchTrainer_99599_00000_0_2024-10-03_01-11-04/checkpoint_000000"}
{"training_id": "4b4ede7f", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 4, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 8, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/4b4ede7f/TorchTrainer_2024-10-03_01-11-04/TorchTrainer_99599_00000_0_2024-10-03_01-11-04/checkpoint_000003"}
{"training_id": "4b4ede7f", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 4, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 8, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/4b4ede7f/TorchTrainer_2024-10-03_01-11-04/TorchTrainer_99599_00000_0_2024-10-03_01-11-04/checkpoint_000001"}
{"training_id": "4b4ede7f", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "/pub/scratch/xiayao/projects/utils/trainer/data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 4, "num_checkpoints_to_keep": null, "lr": 5e-05, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 8, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 8, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/4b4ede7f/TorchTrainer_2024-10-03_01-11-04/TorchTrainer_99599_00000_0_2024-10-03_01-11-04/checkpoint_000002"}
{"training_id": "2a0abd87", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/2a0abd87/TorchTrainer_2024-10-06_07-58-51/TorchTrainer_0fd76_00000_0_2024-10-06_07-58-51/checkpoint_000002"}
{"training_id": "2a0abd87", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/2a0abd87/TorchTrainer_2024-10-06_07-58-51/TorchTrainer_0fd76_00000_0_2024-10-06_07-58-51/checkpoint_000001"}
{"training_id": "2a0abd87", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/2a0abd87/TorchTrainer_2024-10-06_07-58-51/TorchTrainer_0fd76_00000_0_2024-10-06_07-58-51/checkpoint_000004"}
{"training_id": "2a0abd87", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/2a0abd87/TorchTrainer_2024-10-06_07-58-51/TorchTrainer_0fd76_00000_0_2024-10-06_07-58-51/checkpoint_000003"}
{"training_id": "2a0abd87", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/2a0abd87/TorchTrainer_2024-10-06_07-58-51/TorchTrainer_0fd76_00000_0_2024-10-06_07-58-51/checkpoint_000000"}
{"training_id": "ce62329e", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/ce62329e/TorchTrainer_2024-10-05_04-51-27/TorchTrainer_b7a4f_00000_0_2024-10-05_04-51-27/checkpoint_000003"}
{"training_id": "ce62329e", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/ce62329e/TorchTrainer_2024-10-05_04-51-27/TorchTrainer_b7a4f_00000_0_2024-10-05_04-51-27/checkpoint_000004"}
{"training_id": "ce62329e", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/ce62329e/TorchTrainer_2024-10-05_04-51-27/TorchTrainer_b7a4f_00000_0_2024-10-05_04-51-27/checkpoint_000000"}
{"training_id": "ce62329e", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/ce62329e/TorchTrainer_2024-10-05_04-51-27/TorchTrainer_b7a4f_00000_0_2024-10-05_04-51-27/checkpoint_000002"}
{"training_id": "ce62329e", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/ce62329e/TorchTrainer_2024-10-05_04-51-27/TorchTrainer_b7a4f_00000_0_2024-10-05_04-51-27/checkpoint_000001"}
{"training_id": "26e76f52", "epoch": 0, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/26e76f52/TorchTrainer_2024-10-05_01-51-10/TorchTrainer_88317_00000_0_2024-10-05_01-51-10/checkpoint_000000"}
{"training_id": "26e76f52", "epoch": 4, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/26e76f52/TorchTrainer_2024-10-05_01-51-10/TorchTrainer_88317_00000_0_2024-10-05_01-51-10/checkpoint_000004"}
{"training_id": "26e76f52", "epoch": 3, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/26e76f52/TorchTrainer_2024-10-05_01-51-10/TorchTrainer_88317_00000_0_2024-10-05_01-51-10/checkpoint_000003"}
{"training_id": "26e76f52", "epoch": 1, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/26e76f52/TorchTrainer_2024-10-05_01-51-10/TorchTrainer_88317_00000_0_2024-10-05_01-51-10/checkpoint_000001"}
{"training_id": "26e76f52", "epoch": 2, "ft_config": {"mx": "bf16", "batch_size_per_device": 16, "stop_perplexity": 0, "eval_batch_size_per_device": 64, "num_devices": 2, "grad_accum": 1, "train_path": "/pub/scratch/xiayao/projects/utils/trainer/data/train.jsonl", "test_path": "/pub/scratch/xiayao/projects/utils/trainer/data/test.jsonl", "special_token_path": "./data/tokens.json", "no_grad_ckpt": false, "output_dir": "outputs/", "model_name": "meta-llama/Llama-2-7b-hf", "num_epochs": 5, "num_checkpoints_to_keep": null, "lr": 1e-06, "ctx_len": 512, "as_test": false, "ds_config": "deepspeed_configs/zero_3_llama_2_7b.json", "lora": true, "lora_target_modules": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj", "lora_rank": 256, "seed": 42, "batch_size": 16, "gradient_accumulation_steps": 1, "block_size": 512, "eval_batch_size": 64, "lora_config": {"r": 256, "lora_alpha": 16, "lora_dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "task_type": "CAUSAL_LM", "modules_to_save": [], "bias": "none", "fan_in_fan_out": false, "init_lora_weights": true}}, "ckpt_path": "outputs/26e76f52/TorchTrainer_2024-10-05_01-51-10/TorchTrainer_88317_00000_0_2024-10-05_01-51-10/checkpoint_000002"}
